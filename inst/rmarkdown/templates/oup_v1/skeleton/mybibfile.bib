
@book{american2013acsm,
  title = {{{ACSM}}'s {{Guidelines}} for {{Exercise Testing}} and {{Prescription}}},
  author = {{American College of Sports Medicine et al.}},
  year = {2013},
  month = mar,
  publisher = {{Lippincott Williams \& Wilkins}},
  abstract = {The flagship title of the certification suite from the American College of Sports Medicine, ACSM's Guidelines for Exercise Testing and Prescription is a handbook that delivers scientifically based standards on exercise testing and prescription to the certification candidate, the professional, and the student. The 9th edition focuses on evidence-based recommendations that reflect the latest research and clinical information. This manual is an essential resource for any health/fitness and clinical exercise professional, physician, nurse, physician assistant, physical and occupational therapist, dietician, and health care administrator. This manual gives succinct summaries of recommended procedures for exercise testing and exercise prescription in healthy and diseased patients.},
  googlebooks = {hhosAwAAQBAJ},
  isbn = {978-1-4698-2666-0},
  keywords = {Medical / Sports Medicine},
  language = {en}
}

@article{bahdanau2014neural,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2016},
  month = may,
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arXiv},
  eprint = {1409.0473},
  eprinttype = {arxiv},
  journal = {arXiv:1409.0473 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{horvath2018dna,
  title = {{{DNA}} Methylation-Based Biomarkers and the Epigenetic Clock Theory of Ageing},
  author = {Horvath, Steve and Raj, Kenneth},
  year = {2018},
  month = jun,
  volume = {19},
  pages = {371--384},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0064},
  doi = {10.1038/s41576-018-0004-3},
  abstract = {Identifying and validating molecular targets of interventions that extend the human health span and lifespan has been difficult, as most clinical biomarkers are not sufficiently representative of the fundamental mechanisms of ageing to serve as their indicators. In a recent breakthrough, biomarkers of ageing based on DNA methylation data have enabled accurate age estimates for any tissue across the entire life course. These `epigenetic clocks' link developmental and maintenance processes to biological ageing, giving rise to a unified theory of life course. Epigenetic biomarkers may help to address long-standing questions in many fields, including the central question: why do we age?},
  copyright = {2018 Macmillan Publishers Ltd., part of Springer Nature},
  journal = {Nature Reviews Genetics},
  language = {en},
  number = {6}
}

@article{imboden2018cardiorespiratory,
  title = {Cardiorespiratory {{Fitness}} and {{Mortality}} in {{Healthy Men}} and {{Women}}},
  author = {{Imboden Mary T.} and {Harber Matthew P.} and {Whaley Mitchell H.} and {Finch W. Holmes} and {Bishop Derron L.} and {Kaminsky Leonard A.}},
  year = {2018},
  month = nov,
  volume = {72},
  pages = {2283--2292},
  publisher = {{American College of Cardiology Foundation}},
  doi = {10.1016/j.jacc.2018.08.2166},
  journal = {Journal of the American College of Cardiology},
  number = {19}
}

@article{ji20123d,
  title = {{{3D Convolutional Neural Networks}} for {{Human Action Recognition}}},
  author = {Ji, Shuiwang and Xu, Wei and Yang, Ming and Yu, Kai},
  year = {2013},
  month = jan,
  volume = {35},
  pages = {221--231},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2012.59},
  abstract = {We consider the automated recognition of human actions in surveillance videos. Most current methods build classifiers based on complex handcrafted features computed from the raw inputs. Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs. However, such models are currently limited to handling 2D inputs. In this paper, we develop a novel 3D CNN model for action recognition. This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels. To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models. We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods.},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {3D convolution,action recognition,Computational modeling,Computer architecture,convolutional neural networks,Deep learning,Feature extraction,Kernel,model combination,Solid modeling,Three dimensional displays,Videos},
  number = {1}
}

@article{krizhevsky2012imagenet,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2017},
  month = may,
  volume = {60},
  pages = {84--90},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3065386},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ``dropout'' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  journal = {Communications of the ACM},
  language = {en},
  number = {6}
}

@article{lecun2015deep,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  volume = {521},
  pages = {436--444},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  journal = {Nature},
  language = {en},
  number = {7553}
}

@inproceedings{motiian2017unified,
  title = {Unified {{Deep Supervised Domain Adaptation}} and {{Generalization}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Motiian, Saeid and Piccirilli, Marco and Adjeroh, Donald A. and Doretto, Gianfranco},
  year = {2017},
  pages = {5715--5725}
}

@book{murphy2012machine,
  title = {Machine {{Learning}}: {{A Probabilistic Perspective}}},
  shorttitle = {Machine {{Learning}}},
  author = {Murphy, Kevin P.},
  year = {2012},
  month = sep,
  publisher = {{MIT Press}},
  abstract = {A comprehensive introduction to machine learning that uses probabilistic models and inference as a unifying approach.Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach.The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package\textemdash PMTK (probabilistic modeling toolkit)\textemdash that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.},
  googlebooks = {RC43AgAAQBAJ},
  isbn = {978-0-262-30432-0},
  keywords = {Computers / Artificial Intelligence / General},
  language = {en}
}

@article{pyrkov2018quantitative,
  title = {Quantitative Characterization of Biological Age and Frailty Based on Locomotor Activity Records},
  author = {Pyrkov, Timothy V. and Getmantsev, Evgeny and Zhurov, Boris and Avchaciov, Konstantin and Pyatnitskiy, Mikhail and Menshikov, Leonid and Khodova, Kristina and Gudkov, Andrei V. and Fedichev, Peter O.},
  year = {2018},
  month = oct,
  volume = {10},
  pages = {2973--2990},
  issn = {1945-4589},
  doi = {10.18632/aging.101603},
  abstract = {We performed a systematic evaluation of the relationships between locomotor activity and signatures of frailty, morbidity, and mortality risks using physical activity records from the 2003-2006 National Health and Nutrition Examination Survey (NHANES) and UK BioBank (UKB). We proposed a statistical description of the locomotor activity tracks and transformed the provided time series into vectors representing physiological states for each participant. The Principal Component Analysis of the transformed data revealed a winding trajectory with distinct segments corresponding to subsequent human development stages. The extended linear phase starts from 35-40 years old and is associated with the exponential increase of mortality risks according to the Gompertz mortality law. We characterized the distance traveled along the aging trajectory as a natural measure of biological age and demonstrated its significant association with frailty and hazardous lifestyles, along with the remaining lifespan and healthspan of an individual. The biological age explained most of the variance of the log-hazard ratio that was obtained by fitting directly to mortality and the incidence of chronic diseases. Our findings highlight the intimate relationship between the supervised and unsupervised signatures of the biological age and frailty, a consequence of the low intrinsic dimensionality of the aging dynamics.},
  journal = {Aging (Albany NY)},
  number = {10},
  pmcid = {PMC6224248},
  pmid = {30362959}
}

@article{rahman2019centroidb,
  title = {Centroid of {{Age Neighborhoods}}: {{A New Approach}} to {{Estimate Biological Age}}},
  shorttitle = {Centroid of {{Age Neighborhoods}}},
  author = {Rahman, Syed Ashiqur and Adjeroh, Donald A.},
  year = {2020},
  month = apr,
  volume = {24},
  pages = {1226--1234},
  issn = {2168-2208},
  doi = {10.1109/JBHI.2019.2930938},
  abstract = {Estimation of human biological age is an important and difficult challenge. Different biomarkers and numerous approaches have been studied for biological age prediction, each with its advantages and limitations. In this paper, we propose a new biological age estimation method, and investigate the performance of the new method. We introduce a centroid based approach, using the notion of age neighborhoods. Specifically, we develop a model, based on which we compute biological age using blood biomarkers, by considering the centroid or mediod of specially selected age neighborhoods. Experiments were performed on the National Health and Human Nutrition Examination Survey dataset with biomarkers (21 451 individuals). Compared with current popular methods for biological age prediction, our experiments show that the proposed age neighborhood model results in an improved performance in human biological age estimation.},
  journal = {IEEE Journal of Biomedical and Health Informatics},
  keywords = {age centroid,Age estimation,age medoid,aging,all-cause mortality,bio-markers,biological age,Biomarkers,Blood,Blood pressure,Correlation,Estimation,Informatics},
  number = {4}
}

@article{ravi2016deep,
  title = {Deep {{Learning}} for {{Health Informatics}}},
  author = {Rav{\`i}, Daniele and Wong, Charence and Deligianni, Fani and Berthelot, Melissa and {Andreu-Perez}, Javier and Lo, Benny and Yang, Guang-Zhong},
  year = {2017},
  month = jan,
  volume = {21},
  pages = {4--21},
  issn = {2168-2208},
  doi = {10.1109/JBHI.2016.2636665},
  abstract = {With a massive influx of multimodality data, the role of data analytics in health informatics has grown rapidly in the last decade. This has also prompted increasing interests in the generation of analytical, data driven models based on machine learning in health informatics. Deep learning, a technique with its foundation in artificial neural networks, is emerging in recent years as a powerful tool for machine learning, promising to reshape the future of artificial intelligence. Rapid improvements in computational power, fast data storage, and parallelization have also contributed to the rapid uptake of the technology in addition to its predictive power and ability to generate automatically optimized high-level features and semantic interpretation from the input data. This article presents a comprehensive up-to-date review of research employing deep learning in health informatics, providing a critical analysis of the relative merit, and potential pitfalls of the technique as well as its future outlook. The paper mainly focuses on key applications of deep learning in the fields of translational bioinformatics, medical imaging, pervasive sensing, medical informatics, and public health.},
  journal = {IEEE Journal of Biomedical and Health Informatics},
  keywords = {Artificial neural networks,Bioinformatics,Biological neural networks,Biomedical imaging,deep learning,health informatics,Informatics,machine learning,Machine learning,medical imaging,Neurons,public health,Training,wearable devices},
  number = {1}
}

@inproceedings{wang2018face,
  title = {Face {{Aging With Identity}}-{{Preserved Conditional Generative Adversarial Networks}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Wang, Zongwei and Tang, Xu and Luo, Weixin and Gao, Shenghua},
  year = {2018},
  pages = {7939--7947}
}

@article{zhang2018fine,
  title = {Fine-{{Grained Age Estimation}} in the {{Wild With Attention LSTM Networks}}},
  author = {Zhang, Ke and Liu, Na and Yuan, Xingfang and Guo, Xinyao and Gao, Ce and Zhao, Zhenbing and Ma, Zhanyu},
  year = {2020},
  month = sep,
  volume = {30},
  pages = {3140--3152},
  issn = {1558-2205},
  doi = {10.1109/TCSVT.2019.2936410},
  abstract = {Age estimation from a single face image has been an essential task in the field of human-computer interaction and computer vision, which has a wide range of practical application values. Accuracy of age estimation of face images in the wild is relatively low for existing methods, because they only take into account the global features, while neglecting the fine-grained features of age-sensitive areas. We propose a novel method based on our attention long short-term memory (AL) network for fine-grained age estimation in the wild, inspired by the fine-grained categories and the visual attention mechanism. This method combines the residual networks (ResNets) or the residual network of residual network (RoR) models with LSTM units to construct AL-ResNets or AL-RoR networks to extract local features of age-sensitive regions, which effectively improves the age estimation accuracy. First, a ResNets or a RoR model pretrained on ImageNet dataset is selected as the basic model, which is then fine-tuned on the IMDB-WIKI-101 dataset for age estimation. Then, we fine-tune the ResNets or the RoR on the target age datasets to extract the global features of face images. To extract the local features of age-sensitive regions, the LSTM unit is then presented to obtain the coordinates of the age-sensitive region automatically. Finally, the age group classification is conducted directly on the Adience dataset, and age-regression experiments are performed by the Deep EXpectation algorithm (DEX) on MORPH Album 2, FG-NET and 15/16LAP datasets. By combining the global and the local features, we obtain our final prediction results. Experimental results illustrate the effectiveness and robustness of the proposed AL-ResNets or AL-RoR for age estimation in the wild, where it achieves better state-of-the-art performance than all other convolutional neural network (CNN) methods on the Adience, MORPH Album 2, FG-NET and 15/16LAP datasets.},
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  keywords = {Aging,attention LSTM networks,Benchmark testing,Estimation,Face,Feature extraction,Fine-grained age estimation,LSTM,Power systems,ResNets,RoR},
  number = {9}
}


